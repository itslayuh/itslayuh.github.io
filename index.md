## Welcome To My Blog




## Dear Data: A week of Sleep and Productivity


![image](https://user-images.githubusercontent.com/55146779/68534011-c860f400-02fd-11ea-9c5a-d608fd5e2181.png)

![image](https://user-images.githubusercontent.com/55146779/68534005-c0a14f80-02fd-11ea-9ef4-36f980cf5213.png)



## Tool Review: Voyant Tools
  Voyant Tools is a free, web-based textual analysis program which enables its users to perform ‘distance readings’, i.e., to analyze large amounts of text (in either a single work or across multiple) in order to plot the frequency and distribution of certain words or phrases. Doing so makes it easier to identify the patterns that may be latent in certain genres of literature, or other kinds of related corpus. The tool was developed by Geoffrey Rockwell and Stefan Sinclair, who in ‘The Measured Words: How Computers Analyze Text’, offer a helpful overview of how Voyant and other similar programs process the texts fed to them by users. By treating bits of text (words or phrases) as ‘strings’, Voyant can be used “to manipulate [a text] in ways that can be anticipated” by the user. Strings are essentially “sequences of characters that have a beginning and end” which help a program to identify what information it should attempt to represent .
	
  For the purpose of textual analysis, Voyant has an edge over similar kinds of tools both in the fact that it is open-access, and in its relative simplicity. The platform’s user interface is fairly straightforward, allowing users to upload text files from their computer, or simply paste in the URLs which contain the texts of interest. Once this data has been inputted, Voyant displays a variety of customizable visualizations (e.g., charts, graphs, word clouds, etc) with which the user can detect trends in the frequency and distribution of words. If a user doesn’t quite understand what the visualizations are meant to say about their texts, Voyant provides helpful explanations accessible by clicking on the question mark symbol. The relative ease with which one can access and use Voyant makes it such that even those who are new to large-scale textual analysis can utilize the platform to yield helpful insights about their corpus. 
	
  Critique of Voyant (and other similar platforms) inevitably comes down less on the nature of the platform itself, and more on what it enables users to do, i.e., to perform ‘distance readings’. The debate about the value of distance reading (over the traditional practice of close reading) in the humanities is ongoing, with some arguing that an important degree of nuance is lost when forgoing focused analysis of individual texts for large-scale quantitative mapping. Additionally, some--such as Kathryn Schulz--maintain that “literature is an artificial universe, and the written word, unlike the natural world, can’t be counted on to obey a set of laws”. Schultz’s view does beget the question of why someone interested in--for example--a work of Victorian fiction, even want to analyze, “as many as 60,000 other novels [which] were published in 19th-century England”? In response, Franco Moretti suggests that we (as humanities scholars) ought to recognize the value in distance reading “because its opposite, close reading, can’t uncover the true scope and nature of literature”. Although this might be taken as a contentious claim, if one’s intention is indeed to analyze an inexorably wide swathe of texts, Voyant is an excellent tool for this purpose. 
	
  Granted, as Rockwell and Sinclair rightly point out, “computers cannot yet begin to approximate [the] graduate level interpretive skills” that people use when analyzing individual texts. However, this fact is not odds with what tools such as voyant aim to do, i.e., make the process of interpretation significantly easier for one tasked with analyzing a vast corpus of texts. Voyant cannot replace human beings in our role of informed, qualitative analyzers; it can however, enable us to engage with a vast body of textual works in a significantly shorter amount of time than we would be able to otherwise. Used for this particular purpose, Voyant performs exceptionally well.
  
  Sources:
  1.  Rockwell, Geoffrey, and Stéfan Sinclair. “The Measured Words: How Computers Analyze Text.” Hermeneutica, 2016, 25–44. https://doi.org/10.7551/mitpress/9780262034357.003.0002.
2. Schulz, Kathryn. “What Is Distant Reading?” The New York Times. The New York Times, June 24, 2011. https://www.nytimes.com/2011/06/26/books/review/the-mechanic-muse-what-is-distant-reading.html?mtrref=www.google.com&assetType=REGIWALL.

## Tool Review: Gephi
Gephi is an open source data visualization and analysis software developed by Sébastien Heymann, Mathieu Bastian, and other students at the University of Technology of Compiègne. Initially released in 2008, Gephi “uses a 3D render engine to display large networks in real-time and to speed up the exploration” of the graphs it produces. It also allows users to “import, visualize, spatialize, filter, manipulate and export all types of networks”, where ‘networks’ refer to the relationships between various objects. Visualizing networks is a useful analytical practice “to leverage the perceptual abilities of humans to find features in network structure and data”. Gephi makes network visualization fairly accessible to users of all levels of experience with topic modeling, by virtue of being free, and through the inclusion of several getting-started tutorials on its website. While there are other open-source programs with similar capabilities to Gephi (such as Cytoscape, Graph-tool, AllegroGraph, and others), it stands apart from the rest through the relative simplicity of its user interface. 

Gephi has a considerable range of potential analytical applications for both the digital humanities and for other humanities-based disciplines; in particular, when applied to large corpuses, Gephi distinguishes itself as a powerful and highly accessible tool for data visualization. Another promising application for the program is in regards to the practice of ‘topic modelling’. Topic models help to streamline the process of analyzing large corpuses by visualizing the frequency of certain words and phrases, as well as potential connections between them. ‘Topics’ are “cluster of words that frequently occur together”, and through “contextual clues, topic models can connect words with similar meanings and distinguish between uses of words with multiple meanings”. In theory, topic modelling software might appear apt to replace certain traditional methods of textual analysis (e.g, close-readings). However, in practice, tools such as gephi often fail to deliver useful insights when applied to smaller bodies of text.

Topic modelling almost inevitably involves “an attempt to inject semantic meaning into [the] vocabulary”of a body of text; this is immensely valuable to one working with an impossibly large corpus. However, when working with a quantity of text that could feasibly be read and analyzed by an individual, the use of tools such as Gephi that are designed for large-scale textual analysis begets the risk of missing important nuance. Additionally, it must be noted that “topic modeling programs do not know anything about the meaning of the words in a text”, and thus do not render the input, guidance, and qualitative analysis of humans obsolete. Like most large-scale textual analysis tools available to humanists, Gephi is symptomatic--rather than causal--of the shift in the humanities towards the digital. 

Sources
https://en.wikipedia.org/wiki/Gephi, https://www.aaai.org/ocs/index.php/ICWSM/09/paper/view/154, https://gephi.org/, http://mallet.cs.umass.edu/topics.php, https://programminghistorian.org/en/lessons/topic-modeling-and-mallet

