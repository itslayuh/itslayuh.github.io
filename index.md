## Welcome To My Blog


## Final Project: A Semantic Network of Funding for the Digital Humanities
![image](https://i.imgur.com/ieZ8zPw.png)

What--if any--is the relationship between neoliberalism and the humanities? Some academics have argued that increase in funding for technology-oriented education represents an assault by external, corporate entities on traditional forms of humanities scholarship. Moreover, some--such as Richard Grusin--have suggested that “it is no coincidence that the digital humanities has emerged as ‘the next big thing’ at the same moment that the neoliberalization and corporatization of higher education has intensified in the first decades of the twenty-first century”. Others have argued against claims such as Grusin’s, e.g., Franco Moretti, who has stated plainly that “the general idea that the digital humanities are aligned with the big tech companies is simply not true”. A direct link between neoliberal interests and the kinds of funding that universities have received in recent times is hard to prove definitively; however, when stepping back to look at broader shifts in what kind of academic pursuits are deemed ‘valuable’, certain concerning patterns do seem to emerge.
	
To illustrate these trends and to explore the controversy from a different angle, I used two open-source digital tools (viz., Voyant Tools and Gephi) to create a semantic network from a relevant corpus of text. Specifically, I looked to recent news about changes in funding for the digital humanities at academic institutions, and sought to visualize a network of connected and/or frequently occurring terms. The data I collected for my semantic network was gleaned from the news pages for various colleges and universities; In particular, I was interested in articles which discussed the implementation and/or expansion of the digital humanities in academic contexts. As I skimmed through each article, I began to notice certain commonalities and trends between each, especially in regards to the kinds and sources of funding for digital humanities interests at each institution.

To make the task of analyzing a fairly large corpus easier and more expedient, I used Voyant Tools to perform a distance reading of 20 articles. As expected from prior experience, Voyant made this aspect of the process fairly straightforward; all that inputting my dataset involved was copy-pasting the link for each article into the platform. Once I submitted my data, voyant offered a plethora of useful options for analyzing and displaying the trends it unearthed. In the interest of converting my dataset into a format that would be compatible with Gephi (i.e, a spreadsheet with comma separated values), I used the ‘Corpus Collocates’ tool, which provides “a table view of which terms appear more frequently in proximity to keywords across the entire corpus”. Having narrowed down my corpus to around 70 unique terms which co-occurred frequently, I then attempted to convert my data into a workable format for Gephi in a CSV spreadsheet.

I ran into a considerable amount of unexpected difficulty with this process, primarily in regards to properly organizing my node and edge tables in order to produce the right kind of visualization. There are many detailed guides and tutorials online for how one ought to go about this, but most seem to require a baseline level of knowledge about data that I didn’t have. Although I did eventually figure out what exactly needed to be in the spreadsheets, the process proved to be frustrating, and it took me a long time to manually transform each collocate term into a unique ‘Id’, or node. Regardless, the process of trial and error was a valuable learning experience, finally getting gephi to show me the visualization I was after was highly gratifying.
	Initially, I intended the visualization of my data to be illustrative of the specific sources and nature of funding for digital humanities departments at academic institutions. Skimming each article, I noticed that certain names occurred more frequently than others, e.g., the Andrew W. Mellon Foundation, which seemed to be the most prolific funder of technology-oriented academic initiatives. However, as previously mentioned, I had quite a bit of difficulty preparing my data spreadsheets correctly for gephi. One such difficulty was that, if I had included the names of each foundation and/or scholarship in conjunction with their collocates, the resulting graph would be so large and complex that it would defeat the purpose of using a visualization for semantic simplicity. Thus, I decided to forgo specific names and focus instead on collocates for more broadly applicable terms. 
	
The resulting visualization is organized by the frequency of connections that each term had to another, with the largest nodes representing the most connected terms, and the smallest nodes representing what the larger was most often mentioned in context with. Unsurprisingly, the terms ‘digital’ and ‘humanities’ were the most well-connected nodes, followed by ‘faculty’, ‘research’, ‘students’, ‘university’, ‘projects’ and ‘grant’. What does this say (if anything) about neoliberalism and the humanities? As previously mentioned, this kind of visualization does not prove a direct, tangible link between corporate interests and academic funding; however, it does (in my opinion) showcase how this funding is being used by academic institutions. For example, some of the largest nodes (digital, humanities, research, and grant) are all connected to one another, and each respectively is connected to terms that suggest what universities are doing in the humanities. 

![image](https://i.imgur.com/v9vMUQy.png)

For example, that funding is used for things such as ‘labs’, ‘hubs’, ‘technology’, etc., could be taken as indicative of the shift in humanities towards forms of scholarship traditionally associated with the sciences. Is this bad? Not necessarily-- the application of technological methods in the humanities does not have to entail that traditional forms of humanistic scholarship (such as close-reading) will be rendered obsolete. What academics do with the digital humanities is up to them; funding can certainly steer institutions in particular directions, but it does not force them to abandon the core interests of the humanities. 


The Dataset:
http://www.thedp.com/article/2019/11/digital-humanities-programming-course-restructure-penn
http://now.etown.edu/index.php/2019/11/08/combining-technology-and-humanities-in-the-digital-humanities-hub/
https://www.shu.edu/arts-sciences/news/digital-humanities-and-the-liberal-arts-curriculum.cfm
https://dailyegyptian.com/97866/uncategorized/out-with-ye-olde-and-in-with-the-new-enrollment-in-english-increases/
http://news.mit.edu/2019/mit-program-digital-humanities-launches-with-mellon-grant-0417
https://almanac.upenn.edu/articles/2-million-mellon-grant-to-the-price-lab
https://yaledailynews.com/blog/2018/09/27/new-digital-humanities-lab-to-open/
https://news.vanderbilt.edu/2019/03/15/mellon-foundation-renews-support-for-center-for-digital-humanities/
https://newsandfeatures.uncg.edu/digital-humanities-project-325000-neh-grant/
https://www.manchester.ac.uk/discover/news/major-investment-in-new-digital-humanities-image-viewer/
https://www.news.ucsb.edu/2019/019613/future-history
https://columns.wlu.edu/wls-brock-awarded-national-endowment-for-the-humanities-grant/
https://news.stonybrook.edu/oncampus/digital-humanities-find-a-home-with-gladys-brooks-foundation-gift/
https://www.xula.edu/singleArticle?articleId=article___news___exemplary
https://news.uci.edu/2018/12/27/italian-professor-wins-neh-grant/
https://www.drew.edu/news/2017/11/07/mellon-foundation-grant-fuels-digital-drew-initiative
https://humanities.arizona.edu/news/transforming-humanities-through-technology
https://www.baylor.edu/mediacommunications/news.php?action=story&story=189791
https://msutoday.msu.edu/news/2018/msu-uses-15m-mellon-foundation-grant-to-build-massive-slave-trade-database/
https://www.brandeis.edu/now/2018/december/neh-grants.html


## Dear Data: A week of Sleep and Productivity


![image](https://user-images.githubusercontent.com/55146779/68534011-c860f400-02fd-11ea-9c5a-d608fd5e2181.png)

![image](https://user-images.githubusercontent.com/55146779/68534005-c0a14f80-02fd-11ea-9ef4-36f980cf5213.png)



## Tool Review: Voyant Tools
  Voyant Tools is a free, web-based textual analysis program which enables its users to perform ‘distance readings’, i.e., to analyze large amounts of text (in either a single work or across multiple) in order to plot the frequency and distribution of certain words or phrases. Doing so makes it easier to identify the patterns that may be latent in certain genres of literature, or other kinds of related corpus. The tool was developed by Geoffrey Rockwell and Stefan Sinclair, who in ‘The Measured Words: How Computers Analyze Text’, offer a helpful overview of how Voyant and other similar programs process the texts fed to them by users. By treating bits of text (words or phrases) as ‘strings’, Voyant can be used “to manipulate [a text] in ways that can be anticipated” by the user. Strings are essentially “sequences of characters that have a beginning and end” which help a program to identify what information it should attempt to represent .
	
  For the purpose of textual analysis, Voyant has an edge over similar kinds of tools both in the fact that it is open-access, and in its relative simplicity. The platform’s user interface is fairly straightforward, allowing users to upload text files from their computer, or simply paste in the URLs which contain the texts of interest. Once this data has been inputted, Voyant displays a variety of customizable visualizations (e.g., charts, graphs, word clouds, etc) with which the user can detect trends in the frequency and distribution of words. If a user doesn’t quite understand what the visualizations are meant to say about their texts, Voyant provides helpful explanations accessible by clicking on the question mark symbol. The relative ease with which one can access and use Voyant makes it such that even those who are new to large-scale textual analysis can utilize the platform to yield helpful insights about their corpus. 
	
  Critique of Voyant (and other similar platforms) inevitably comes down less on the nature of the platform itself, and more on what it enables users to do, i.e., to perform ‘distance readings’. The debate about the value of distance reading (over the traditional practice of close reading) in the humanities is ongoing, with some arguing that an important degree of nuance is lost when forgoing focused analysis of individual texts for large-scale quantitative mapping. Additionally, some--such as Kathryn Schulz--maintain that “literature is an artificial universe, and the written word, unlike the natural world, can’t be counted on to obey a set of laws”. Schultz’s view does beget the question of why someone interested in--for example--a work of Victorian fiction, even want to analyze, “as many as 60,000 other novels [which] were published in 19th-century England”? In response, Franco Moretti suggests that we (as humanities scholars) ought to recognize the value in distance reading “because its opposite, close reading, can’t uncover the true scope and nature of literature”. Although this might be taken as a contentious claim, if one’s intention is indeed to analyze an inexorably wide swathe of texts, Voyant is an excellent tool for this purpose. 
	
  Granted, as Rockwell and Sinclair rightly point out, “computers cannot yet begin to approximate [the] graduate level interpretive skills” that people use when analyzing individual texts. However, this fact is not odds with what tools such as voyant aim to do, i.e., make the process of interpretation significantly easier for one tasked with analyzing a vast corpus of texts. Voyant cannot replace human beings in our role of informed, qualitative analyzers; it can however, enable us to engage with a vast body of textual works in a significantly shorter amount of time than we would be able to otherwise. Used for this particular purpose, Voyant performs exceptionally well.
  
  Sources:
  1.  Rockwell, Geoffrey, and Stéfan Sinclair. “The Measured Words: How Computers Analyze Text.” Hermeneutica, 2016, 25–44. https://doi.org/10.7551/mitpress/9780262034357.003.0002.
2. Schulz, Kathryn. “What Is Distant Reading?” The New York Times. The New York Times, June 24, 2011. https://www.nytimes.com/2011/06/26/books/review/the-mechanic-muse-what-is-distant-reading.html?mtrref=www.google.com&assetType=REGIWALL.

## Tool Review: Gephi
Gephi is an open source data visualization and analysis software developed by Sébastien Heymann, Mathieu Bastian, and other students at the University of Technology of Compiègne. Initially released in 2008, Gephi “uses a 3D render engine to display large networks in real-time and to speed up the exploration” of the graphs it produces. It also allows users to “import, visualize, spatialize, filter, manipulate and export all types of networks”, where ‘networks’ refer to the relationships between various objects. Visualizing networks is a useful analytical practice “to leverage the perceptual abilities of humans to find features in network structure and data”. Gephi makes network visualization fairly accessible to users of all levels of experience with topic modeling, by virtue of being free, and through the inclusion of several getting-started tutorials on its website. While there are other open-source programs with similar capabilities to Gephi (such as Cytoscape, Graph-tool, AllegroGraph, and others), it stands apart from the rest through the relative simplicity of its user interface. 

Gephi has a considerable range of potential analytical applications for both the digital humanities and for other humanities-based disciplines; in particular, when applied to large corpuses, Gephi distinguishes itself as a powerful and highly accessible tool for data visualization. Another promising application for the program is in regards to the practice of ‘topic modelling’. Topic models help to streamline the process of analyzing large corpuses by visualizing the frequency of certain words and phrases, as well as potential connections between them. ‘Topics’ are “cluster of words that frequently occur together”, and through “contextual clues, topic models can connect words with similar meanings and distinguish between uses of words with multiple meanings”. In theory, topic modelling software might appear apt to replace certain traditional methods of textual analysis (e.g, close-readings). However, in practice, tools such as gephi often fail to deliver useful insights when applied to smaller bodies of text.

Topic modelling almost inevitably involves “an attempt to inject semantic meaning into [the] vocabulary”of a body of text; this is immensely valuable to one working with an impossibly large corpus. However, when working with a quantity of text that could feasibly be read and analyzed by an individual, the use of tools such as Gephi that are designed for large-scale textual analysis begets the risk of missing important nuance. Additionally, it must be noted that “topic modeling programs do not know anything about the meaning of the words in a text”, and thus do not render the input, guidance, and qualitative analysis of humans obsolete. Like most large-scale textual analysis tools available to humanists, Gephi is symptomatic--rather than causal--of the shift in the humanities towards the digital. 

Sources
https://en.wikipedia.org/wiki/Gephi, https://www.aaai.org/ocs/index.php/ICWSM/09/paper/view/154, https://gephi.org/, http://mallet.cs.umass.edu/topics.php, https://programminghistorian.org/en/lessons/topic-modeling-and-mallet

